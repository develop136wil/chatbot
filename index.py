import os
import json
import time
import traceback
import logging
from typing import Dict, Any, List, Optional
from supabase import create_client
from notion_client import Client as NotionClient
from dotenv import load_dotenv
from utils import (
    get_llm_client, # [ìˆ˜ì •] Lazy Loader Import
    summarize_content_with_llm, 
    _get_title, 
    _get_number, 
    _get_rich_text,
    _get_url,
    get_gemini_embedding,
    _get_multi_select
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

logger.info("[Indexer] ì„¤ì • ë¡œë“œ ì¤‘...")
load_dotenv()

# [ìˆ˜ì •] ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” (Lazy Loading)
NOTION_KEY = None
SUPABASE_URL = None
SUPABASE_KEY = None
notion = None
supabase = None

def init_clients():
    global NOTION_KEY, SUPABASE_URL, SUPABASE_KEY, notion, supabase
    
    NOTION_KEY = os.getenv("NOTION_API_KEY", os.getenv("NOTION_KEY"))
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")

    if not NOTION_KEY: 
        logger.critical("âŒ NOTION_KEY ì„¤ì • í•„ìš”")
        raise ValueError("NOTION_KEY ì„¤ì • í•„ìš”")
    if not SUPABASE_URL or not SUPABASE_KEY: 
        logger.critical("âŒ SUPABASE ì„¤ì • í•„ìš”")
        raise ValueError("SUPABASE ì„¤ì • í•„ìš”")

    logger.info("[Indexer] í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
    try:
        notion = NotionClient(auth=NOTION_KEY)
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    except Exception as e:
        logger.critical(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise e

    logger.info("[Indexer] ì´ˆê¸°í™” ì™„ë£Œ.")

DATABASE_IDS = {
    "ì˜ë£Œ/ì¬í™œ": "2738ade5021080b786b0d8b0c07c1ea2",
    "êµìœ¡/ë³´ìœ¡": "2738ade5021080339203d7148d7d943b",
    "ê°€ì¡± ì§€ì›": "2738ade502108041a4c7f5ec4c3b8413",
    "ëŒë´„/ì–‘ìœ¡": "2738ade5021080cf842df820fdbeb709",
    "ìƒí™œ ì§€ì›": "2738ade5021080579e5be527ff1e80b2"
}
NOTION_PROPERTY_NAMES = {
    "title": "ì‚¬ì—…ëª…", "category": "ë¶„ë¥˜", "sub_category": "ëŒ€ìƒ íŠ¹ì„±",
    "start_age": "ì‹œì‘ ì›”ë ¹(ê°œì›”)", "end_age": "ì¢…ë£Œ ì›”ë ¹(ê°œì›”)", "support_detail": "ìƒì„¸ ì§€ì› ë‚´ìš©",
    "contact": "ë¬¸ì˜ì²˜", "url1": "ê´€ë ¨ í™ˆí˜ì´ì§€ 1", "url2": "ê´€ë ¨ í™ˆí˜ì´ì§€ 2",
    "url3": "ê´€ë ¨ í™ˆí˜ì´ì§€ 3", "extra_req": "ì¶”ê°€ ìê²©ìš”ê±´",
    "cost_info": "ë¹„ìš© ë¶€ë‹´", "notes": "ì£¼ì˜ì‚¬í•­"
}

STATE_FILE_PATH = "./chroma-data/indexing_state.json"

def load_state() -> Dict[str, str]:
    if os.path.exists(STATE_FILE_PATH):
        try:
            with open(STATE_FILE_PATH, "r", encoding="utf-8") as f: return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return {}

def save_state(state: Dict[str, str]):
    try:
        os.makedirs(os.path.dirname(STATE_FILE_PATH), exist_ok=True)
        with open(STATE_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"âŒ ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def run_indexing():
    # [ìˆ˜ì •] ì‹¤í–‰ ì‹œì ì— ì´ˆê¸°í™” ìˆ˜í–‰
    init_clients()
    
    logger.info("\nğŸ”¥ğŸ”¥ğŸ”¥ [ì—…ë°ì´íŠ¸] ë¬¸ì„œ ì„ë² ë”©(RETRIEVAL_DOCUMENT) ìµœì í™” ì¸ë±ì‹± ì‹œì‘ ğŸ”¥ğŸ”¥ğŸ”¥\n")
    
    client = get_llm_client()
    if not client:
        logger.critical("âŒ FATAL: Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì¸ë±ì‹±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    prev_state = load_state()  # ì¦ë¶„ ì—…ë°ì´íŠ¸ í™œì„±í™”
    # prev_state = {}          # ì „ì²´ ì¬ì¸ë±ì‹± (í•„ìš”ì‹œ ì‚¬ìš©) 
    current_state = {}
    total_processed = 0
    total_skipped = 0
    has_critical_error = False
    
    for category_name, db_id in DATABASE_IDS.items():
        logger.info(f"\n[Indexer] '{category_name}' DB í™•ì¸ ì¤‘...")
        try:
            results = []
            
            # [ìˆ˜ì • 2] ì•ˆì „í•œ í˜ì´ì§€ë„¤ì´ì…˜(Pagination) ë¡œì§
            has_more = True
            next_cursor = None
            
            while has_more:
                query_params = {"database_id": db_id}
                if next_cursor: query_params["start_cursor"] = next_cursor
                
                try:
                    response = notion.databases.query(**query_params)
                    results.extend(response.get("results", []))
                    has_more = response.get("has_more")
                    next_cursor = response.get("next_cursor")
                    time.sleep(0.3) # API ì†ë„ ì œí•œ ì¤€ìˆ˜
                except Exception as e:
                    logger.error(f"âŒ Notion API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    has_more = False
            
            logger.info(f" - {len(results)}ê°œ í˜ì´ì§€ ë°œê²¬.")

            for page in results:
                page_id = page.get("id")
                last_edited = page.get("last_edited_time")
                if not page_id: continue
                
                current_state[page_id] = last_edited

                if page_id in prev_state and prev_state[page_id] == last_edited:
                    total_skipped += 1
                    continue

                logger.info(f"âš¡ï¸ ì²˜ë¦¬ ì‹œì‘ (ID: {page_id})")

                try:
                    supabase.table("site_pages").delete().eq("page_id", page_id).execute()
                except Exception as e:
                    logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")

                # ë°ì´í„° ì¶”ì¶œ
                try:
                    props = page.get("properties", {})
                    title = _get_title(props, NOTION_PROPERTY_NAMES["title"])
                    support_detail = _get_rich_text(props, NOTION_PROPERTY_NAMES["support_detail"])
                    extra_req = _get_rich_text(props, NOTION_PROPERTY_NAMES["extra_req"])
                    contact = _get_rich_text(props, NOTION_PROPERTY_NAMES["contact"])
                    
                    cost_info = _get_rich_text(props, NOTION_PROPERTY_NAMES["cost_info"]) if NOTION_PROPERTY_NAMES.get("cost_info") in props else ""
                    notes = _get_rich_text(props, NOTION_PROPERTY_NAMES["notes"]) if NOTION_PROPERTY_NAMES.get("notes") in props else ""
                    page_url = page.get("url", "")
                    
                    start_age = _get_number(props, NOTION_PROPERTY_NAMES["start_age"])
                    end_age = _get_number(props, NOTION_PROPERTY_NAMES["end_age"])
                    if end_age == -1: end_age = 99999
    
                    targets = _get_multi_select(props, NOTION_PROPERTY_NAMES["sub_category"])
                    targets_text = ", ".join(targets) if targets else ""
                    
                    age_text = ""
                    if start_age is not None and start_age != -1:
                        if end_age is not None and end_age != 99999: age_text = f"{int(start_age)}~{int(end_age)}ê°œì›”"
                        else: age_text = f"{int(start_age)}ê°œì›” ì´ìƒ"
                    elif end_age is not None and end_age != 99999: age_text = f"~{int(end_age)}ê°œì›”"
                    
                    final_target = f"{age_text} ({targets_text})" if targets_text else age_text
    
                    # [1] ìš”ì•½ìš© í…ìŠ¤íŠ¸
                    text_parts = [
                        f"ì‚¬ì—…ëª…: {title}",
                        f"ëŒ€ìƒ: {final_target}",
                        support_detail,
                        f"ì¶”ê°€ ìê²©ìš”ê±´: {extra_req}",
                        f"ë¬¸ì˜ì²˜: {contact}",
                        f"ë¹„ìš© ë¶€ë‹´: {cost_info}" if cost_info and cost_info != "â€”" else "",
                        f"ì£¼ì˜ì‚¬í•­: {notes}" if notes and notes != "â€”" else ""
                    ]
                    full_text_for_summary = "\n".join([p.strip() for p in text_parts if p and p.strip()])
    
                    # [2] ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ (ê°€ì¤‘ì¹˜ ì ìš©)
                    search_keywords = f"{title} {category_name} {targets_text}".replace(" ", ", ")
                    req_text = f"ìê²©ìš”ê±´: {extra_req}" if extra_req and extra_req != "â€”" else ""
                    
                    weight_title = 3
                    weight_target = 2
                    weight_req = 1
                    weight_cost = 2
                    
                    title_repeats = [f"ë¬¸ì„œì œëª©: {title}" for _ in range(weight_title)]
                    target_repeats = [f"ëŒ€ìƒíŠ¹ì„±: {targets_text}" for _ in range(weight_target)] if targets_text else []
                    req_repeats = [f"ìê²©ìš”ê±´: {req_text}" for _ in range(weight_req)] if req_text else []
                    cost_repeats = [f"ë¹„ìš©ì£¼ì˜: {cost_info} {notes}" for _ in range(weight_cost)] if (cost_info and cost_info != "â€”") or (notes and notes != "â€”") else []
                    
                    embedding_parts = [
                        f"í•µì‹¬í‚¤ì›Œë“œ: {search_keywords}",
                        f"ì¹´í…Œê³ ë¦¬: {category_name}",
                        f"ëŒ€ìƒ: {final_target}",
                        f"ë‚´ìš©: {support_detail}",
                    ] + title_repeats + target_repeats + req_repeats + cost_repeats
                    
                    full_text_for_embedding = "\n".join([p.strip() for p in embedding_parts if p and p.strip()])
                except Exception as e:
                    logger.error(f"âŒ ë°ì´í„° íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (ID:{page_id}): {e}")
                    continue

                if total_processed == 0: 
                     logger.debug(f"ğŸ” [X-RAY] ê°€ì¤‘ì¹˜ ì ìš©ëœ ê²€ìƒ‰ ë°ì´í„° ì˜ˆì‹œ:\n{full_text_for_embedding[:300]}...")
                
                # ì²­í¬ ì²˜ë¦¬ ë° ì €ì¥
                chunks = [full_text_for_summary] 
                records_to_insert = []
                
                for i, chunk_text in enumerate(chunks):
                    if len(chunk_text.strip()) < 10: continue
                    chunk_id = f"{page_id}_{i}"

                    logger.info(f"   ... ìš”ì•½ ë° ì„ë² ë”© ìƒì„± ì¤‘ ('{title}')")
                    
                    try:
                        # 1. ìš”ì•½
                        pre_summary = summarize_content_with_llm(chunk_text, title, [])

                        # 2. ì„ë² ë”©
                        embedding = get_gemini_embedding(
                            full_text_for_embedding, 
                            task_type="RETRIEVAL_DOCUMENT"
                        )

                        if not embedding:
                            logger.warning(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨! ê±´ë„ˆëœ€.")
                            continue

                        metadata = {
                            "page_id": page_id,
                            "category": category_name,
                            "sub_category_list": targets,
                            "start_age": start_age,
                            "end_age": end_age,
                            "title": title,
                            "page_url": page_url,
                            "pre_summary": pre_summary
                        }

                        records_to_insert.append({
                            "id": chunk_id,
                            "page_id": page_id,
                            "content": full_text_for_summary,
                            "metadata": metadata,
                            "embedding": embedding
                        })
                    except Exception as e:
                        logger.error(f"âŒ LLM/ì„ë² ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                if records_to_insert:
                    try:
                        supabase.table("site_pages").upsert(records_to_insert).execute()
                        total_processed += 1
                    except Exception as e:
                        logger.error(f"âŒ Supabase ì €ì¥ ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ '{category_name}' ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            has_critical_error = True

    # ì‚­ì œ ì²˜ë¦¬ ë¡œì§
    if has_critical_error:
        logger.warning("\n[Indexer] âš ï¸ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì‚­ì œ ë‹¨ê³„ ê±´ë„ˆëœ€.")
    else:
        deleted_ids = list(set(prev_state.keys()) - set(current_state.keys()))
        if deleted_ids:
            logger.info(f"\n[Indexer] ğŸ—‘ï¸ ì‚­ì œëœ í˜ì´ì§€ {len(deleted_ids)}ê±´ ì •ë¦¬ ì¤‘...")
            for del_id in deleted_ids:
                try:
                    supabase.table("site_pages").delete().eq("page_id", del_id).execute()
                except Exception as e:
                    logger.warning(f"âš ï¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        save_state(current_state)
        logger.info(f"\n[Indexer] âœ¨ ì™„ë£Œ. (ì—…ë°ì´íŠ¸: {total_processed}, ê±´ë„ˆëœ€: {total_skipped})")

if __name__ == "__main__":
    run_indexing()
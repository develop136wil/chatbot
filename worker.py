import os
import json
import time
import traceback
import gc
import logging
from typing import List, Dict, Any, Tuple, Optional
from supabase import create_client
from dotenv import load_dotenv

# ê¸°ë³¸ utils ì„í¬íŠ¸
try:
    from utils import (
        search_supabase,       
        expand_search_query,   
        rerank_search_results, 
        format_search_results, 
        get_llm_client,
        generate_content_safe,
        redis_client,
        supabase,
        notion
    )
    print("âœ… utils ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ utils ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    # Vercelì—ì„œëŠ” sys.exit() ëŒ€ì‹  ì—ëŸ¬ë¥¼ ê¸°ë¡í•˜ê³  ê³„ì† ì§„í–‰
    logger.error(f"Utils import failed: {e}")
    search_supabase = None
    expand_search_query = None
    rerank_search_results = None
    format_search_results = None
    get_llm_client = None
    generate_content_safe = None
    redis_client = None
    supabase = None
    notion = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

logger.info("[Worker] ì„¤ì • ë¡œë“œ ì¤‘...")
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
JOB_QUEUE_KEY = "chatbot:job_queue"
JOB_RESULTS_KEY = "chatbot:job_results"
NOTION_LOG_DB_ID = "2bf8ade502108000b6d6f4ad4d4d52b2"

logger.info("[Worker] í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    logger.error(f"[Worker] Supabase ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    supabase = None

logger.info("[Worker] ì´ˆê¸°í™” ì™„ë£Œ. ì‘ì—… ëŒ€ê¸° ì‹œì‘.")

# [ê¸°ì¡´ ìœ ì§€] ê³ ì • ë©˜íŠ¸ ë‹¤êµ­ì–´ ì‚¬ì „
UI_TRANSLATIONS = {
    "ko": {
        "header_found": "ğŸ” <b>ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!</b>",
        "footer_more": "<p>ğŸ” <b>ì•„ì§ ê²°ê³¼ê°€ ë” ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.</b> 'ë” ë³´ì—¬ì¤˜' ë˜ëŠ” 'ë‹¤ìŒ'ì„ ì…ë ¥í•´ ë³´ì„¸ìš”.</p>",
        "cats": {} 
    },
    "en": {
        "header_found": "ğŸ” <b>Here is the information I found!</b>",
        "footer_more": "<p>ğŸ” <b>There are more results.</b> Try typing 'Show more' or 'Next'.</p>",
        "cats": {
            "ì˜ë£Œ/ì¬í™œ": "Medical/Rehab", "êµìœ¡/ë³´ìœ¡": "Edu/Care", "ê°€ì¡± ì§€ì›": "Family Support",
            "ëŒë´„/ì–‘ìœ¡": "Childcare", "ìƒí™œ ì§€ì›": "Living Support", "ê¸°íƒ€": "Others"
        }
    },
    "vi": {
        "header_found": "ğŸ” <b>TÃ´i Ä‘Ã£ tÃ¬m tháº¥y thÃ´ng tin!</b>",
        "footer_more": "<p>ğŸ” <b>Váº«n cÃ²n káº¿t quáº£.</b> HÃ£y thá»­ nháº­p 'Xem thÃªm' hoáº·c 'Tiáº¿p theo'.</p>",
        "cats": {
            "ì˜ë£Œ/ì¬í™œ": "Y táº¿/PHCN", "êµìœ¡/ë³´ìœ¡": "GiÃ¡o dá»¥c/TrÃ´ng tráº»", "ê°€ì¡± ì§€ì›": "Há»— trá»£ gia Ä‘Ã¬nh",
            "ëŒë´„/ì–‘ìœ¡": "ChÄƒm sÃ³c", "ìƒí™œ ì§€ì›": "Há»— trá»£ Ä‘á»i sá»‘ng", "ê¸°íƒ€": "KhÃ¡c"
        }
    },
    "zh": {
        "header_found": "ğŸ” <b>ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼</b>",
        "footer_more": "<p>ğŸ” <b>è¿˜æœ‰æ›´å¤šç»“æœã€‚</b> è¯·è¾“å…¥â€œæ›´å¤šâ€æˆ–â€œä¸‹ä¸€ä¸ªâ€ã€‚</p>",
        "cats": {
            "ì˜ë£Œ/ì¬í™œ": "åŒ»ç–—/åº·å¤", "êµìœ¡/ë³´ìœ¡": "æ•™è‚²/ä¿è‚²", "ê°€ì¡± ì§€ì›": "å®¶åº­æ”¯æŒ",
            "ëŒë´„/ì–‘ìœ¡": "ç…§æŠ¤/å…»è‚²", "ìƒí™œ ì§€ì›": "ç”Ÿæ´»æ”¯æŒ", "ê¸°íƒ€": "å…¶ä»–"
        }
    }
}

# [â˜…ìˆ˜ì •] ì œëª© ì¼ê´„ ë²ˆì—­ í•¨ìˆ˜ (Batch Processing)
def translate_titles_batch(titles: List[str], target_lang_code: str) -> List[str]:
    """
    ì—¬ëŸ¬ ê°œì˜ ì œëª©ì„ í•œ ë²ˆì— ë²ˆì—­í•˜ì—¬ API í˜¸ì¶œ íšŸìˆ˜ë¥¼ 1/Në¡œ ì¤„ì…ë‹ˆë‹¤.
    """
    client = get_llm_client()
    if not titles or not client: return titles
    
    lang_map = {"en": "English", "vi": "Vietnamese", "zh": "Chinese (Simplified)"}
    target_lang = lang_map.get(target_lang_code, "Korean")
    
    # JSON í¬ë§·ì„ ê°•ì œí•˜ì—¬ íŒŒì‹±í•˜ê¸° ì‰½ê²Œ ë§Œë“¦
    prompt = f"""
    Translate the following list of welfare service titles into {target_lang}.
    
    [Input Titles]
    {json.dumps(titles, ensure_ascii=False)}
    
    [Rules]
    1. Return ONLY a valid JSON list of strings.
    2. Maintain the exact same order.
    3. No explanations, no markdown code blocks. Just the raw JSON list.
    
    [Output Example]
    ["Translated Title 1", "Translated Title 2"]
    """
    
    try:
        # íƒ€ì„ì•„ì›ƒ 40ì´ˆ (ë‚´ìš©ì´ ì¢€ ë” ë§ìœ¼ë¯€ë¡œ)
        response = generate_content_safe(client, prompt, timeout=40)
        
        # [ìˆ˜ì •] ì‘ë‹µ ê°ì²´ ì²˜ë¦¬ ë°©ì‹ í†µì¼
        if hasattr(response, 'text'):
            response_text = response.text.strip()
        else:
            response_text = str(response).strip()
        
        # Markdown code block ì œê±° (`json ... `)
        if response_text.startswith("```"):
            response_text = response_text.split("\n", 1)[1]
            if response_text.endswith("```"):
                response_text = response_text.rsplit("\n", 1)[0]
        
        translated_list = json.loads(response_text)
        
        if isinstance(translated_list, list) and len(translated_list) == len(titles):
            return translated_list
        else:
            logger.warning("âš ï¸ [Batch Translation] ê°œìˆ˜ ë¶ˆì¼ì¹˜ ë˜ëŠ” í¬ë§· ì˜¤ë¥˜. ì›ë³¸ ì œëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return titles
            
    except Exception as e:
        logger.warning(f"âš ï¸ ì œëª© ì¼ê´„ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return titles

# --- ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_job(job_data: Dict[str, Any]) -> Tuple[str, List[str], int]:
    start_time = time.time()
    question = job_data.get("question", "")
    ai_category = job_data.get("ai_category")

    logger.info(f"â–¶ï¸ ì‘ì—… ì‹œì‘: {question}")

    try:
        # [Step 1] í‚¤ì›Œë“œ ì¶”ì¶œ
        try:
            target_keywords = expand_search_query(question)
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ í™•ì¥ ì‹¤íŒ¨: {e}")
            target_keywords = []

        # ì›ë³¸ ì§ˆë¬¸ì˜ ë‹¨ì–´ë„ í‚¤ì›Œë“œì— ì¶”ê°€ (ë³´ì™„ì±…)
        for word in question.split():
            if len(word) > 1 and word not in target_keywords:
                target_keywords.append(word)
        logger.info(f"ğŸ—ï¸ [ê²€ìƒ‰ í‚¤ì›Œë“œ] {target_keywords}")

        # [Step 2] ê²€ìƒ‰
        extracted_info_mock = {"category": ai_category}
        try:
            raw_results = search_supabase(question, extracted_info_mock, keywords=target_keywords)
        except Exception as e:
            logger.error(f"âŒ Supabase ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return "ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜¥", [], 0

        if not raw_results: 
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜¥", [], 0

        # [Step 3] ì¤‘ë³µ ì œê±°
        seen_ids = set()
        unique_results = []
        for doc in raw_results:
            meta = doc.get("metadata", {})
            pid = meta.get("page_id") or meta.get("page_url") or meta.get("title")
            if pid and pid not in seen_ids:
                seen_ids.add(pid)
                unique_results.append(doc)
        candidates = unique_results

        # [Step 4] AI ë­í‚¹
        logger.info(f"ğŸ¤– Geminiì—ê²Œ {len(candidates)}ê°œ ë¬¸ì„œì˜ ë­í‚¹ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
        try:
            reranked_results = rerank_search_results(question, candidates)
            if not reranked_results:
                logger.warning("âš ï¸ AI ë­í‚¹ ê²°ê³¼ ì—†ìŒ -> ê²€ìƒ‰ ì—”ì§„(SQL) ìˆœì„œ ì‚¬ìš©")
                reranked_results = candidates
        except Exception as e:
            logger.error(f"âŒ AI ë­í‚¹ ì¤‘ ì˜¤ë¥˜: {e}")
            reranked_results = candidates

        # [Step 5] ìµœì¢… ê²°ê³¼ ì¡°ë¦½
        display_count = min(len(reranked_results), 2)
        display_results = reranked_results[:display_count]
        
        # ì–¸ì–´ ê°ì§€ ë¡œì§
        target_lang_code = "ko" 
        if "strictly in English" in question: target_lang_code = "en"
        elif "strictly in Vietnamese" in question: target_lang_code = "vi"
        elif "strictly in Chinese" in question: target_lang_code = "zh"
        
        ui_text = UI_TRANSLATIONS.get(target_lang_code, UI_TRANSLATIONS["ko"])

        # ==================================================================
        # [ë‹¤êµ­ì–´ ë²ˆì—­ ì ìš©] ë³¸ë¬¸ + ì¹´í…Œê³ ë¦¬ + â˜…ì œëª©(Batch)â˜…
        # ==================================================================
        if target_lang_code != "ko":
            logger.info(f"ğŸŒ [Worker] ì–¸ì–´ ê°ì§€: {target_lang_code} -> ë‚´ìš©/ì œëª©/UI ë²ˆì—­ ì‹œì‘")
            
            # 1. ì œëª© ì¼ê´„ ìˆ˜ì§‘
            original_titles = [doc.get("metadata", {}).get("title", "") for doc in display_results]
            
            # 2. ì œëª© ì¼ê´„ ë²ˆì—­ ì‹¤í–‰ (1íšŒ í˜¸ì¶œ)
            translated_titles = translate_titles_batch(original_titles, target_lang_code)
            
            # 3. ê²°ê³¼ ì ìš© ë° ë‚˜ë¨¸ì§€ ë²ˆì—­
            for i, doc in enumerate(display_results):
                meta = doc.get("metadata", {})
                original_summary = meta.get("pre_summary", "")
                original_category = meta.get("category", "ê¸°íƒ€")
                original_title = meta.get("title", "")
                
                # 1. ì¹´í…Œê³ ë¦¬ ì´ë¦„ ë²ˆì—­ (ì‚¬ì „ ë§¤í•‘)
                translated_cat = ui_text["cats"].get(original_category, original_category)
                doc["metadata"]["category"] = translated_cat

                # 2. [ì œëª© ë²ˆì—­ ì ìš©] Batch ê²°ê³¼ ì‚¬ìš©
                new_title = translated_titles[i] if i < len(translated_titles) else original_title
                doc["metadata"]["title"] = new_title

                # 3. ë³¸ë¬¸ ìš”ì•½ ë²ˆì—­
                try:
                    translated_summary = summarize_content_with_llm(
                        content=original_summary,  
                        language="ko"
                    )
                    doc["metadata"]["pre_summary"] = translated_summary
                    logger.debug(f"   -> '{original_title}' => '{new_title}' (ë²ˆì—­ ì™„ë£Œ)")
                except Exception as e:
                    logger.warning(f"   âš ï¸ ë³¸ë¬¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        # ==================================================================

        all_page_ids = [r.get("metadata", {}).get("page_id") for r in reranked_results]
        
        final_display_metadata = [res.get("metadata", {}) for res in display_results]
        try:
            body = format_search_results(final_display_metadata)
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            body = "ê²°ê³¼ë¥¼ í‘œì‹œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        header = ui_text["header_found"]
        final_answer = f"{header}<hr>{body}"

        if len(reranked_results) > display_count:
            final_answer += f"<hr>{ui_text['footer_more']}"

        elapsed = time.time() - start_time
        logger.info(f"âœ… ë‹µë³€ ì¡°ë¦½ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        
        # ë¡œê·¸ ì €ì¥ (ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤íŒ¨í•´ë„ ë©”ì¸ ë¡œì§ ì˜í–¥ ì—†ë„ë¡ í•¨)
        if notion and NOTION_LOG_DB_ID:
            try:
                final_category = ai_category if ai_category else "ë¯¸ë¶„ë¥˜"
                notion.pages.create(
                    parent={"database_id": NOTION_LOG_DB_ID},
                    properties={
                        "ì§ˆë¬¸": {"title": [{"text": {"content": question}}]},
                        "ì¹´í…Œê³ ë¦¬": {"select": {"name": final_category}},
                        "í‚¤ì›Œë“œ": {"multi_select": [{"name": k} for k in target_keywords[:5]]}
                    }
                )
            except Exception as e:
                logger.warning(f"âš ï¸ Notion ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
                
        return final_answer, all_page_ids, len(all_page_ids)

    except Exception as e:
        logger.error(f"ğŸ”¥ ì‘ì—… ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜¥", [], 0

# --- ë©”ì¸ ë£¨í”„ ---
def start_worker():
    logger.info(f"ğŸš€ Worker ê°€ë™! (PID: {os.getpid()})")
    
    # Redis ì—°ê²° ì¬ì‹œë„ ë¡œì§
    while True:
        try:
            if redis_client.ping():
                break
        except Exception:
            logger.warning("â³ Redis ì—°ê²° ëŒ€ê¸° ì¤‘...")
            time.sleep(2)
            
    while True:
        try:
            # íƒ€ì„ì•„ì›ƒ 1ì´ˆë¡œ ì„¤ì •í•˜ì—¬ ì£¼ê¸°ì ìœ¼ë¡œ ë£¨í”„ íƒˆì¶œ (ì¢…ë£Œ ì‹œê·¸ë„ ì²˜ë¦¬ ë“± ê°€ëŠ¥)
            result = redis_client.blpop(JOB_QUEUE_KEY, timeout=1)
            if result:
                _, job_json = result
                job_data = json.loads(job_json.decode('utf-8'))
                
                answer_text, all_ids, total_found = process_job(job_data)

                final_result = {
                    "status": "complete",
                    "answer": answer_text,
                    "last_result_ids": all_ids, 
                    "total_found": total_found 
                }
                
                # ê²°ê³¼ ì €ì¥ ì‹œ ë§Œë£Œ ì‹œê°„(TTL) ì„¤ì • ê¶Œì¥ (ì˜ˆ: 1ì‹œê°„)
                job_id = job_data.get("job_id")
                redis_client.hset(JOB_RESULTS_KEY, job_id, json.dumps(final_result).encode('utf-8'))
                # redis_client.expire(f"job:{job_id}", 3600) # (ì„ íƒì‚¬í•­)
                
                logger.info(f"ğŸ’¾ ì™„ë£Œ: {job_data.get('question')}")

                del job_data, answer_text, final_result
                gc.collect()

        except Exception as e:
            logger.error(f"ğŸ”¥ Worker Loop Error: {e}")
            time.sleep(1)

if __name__ == "__main__":
    start_worker()